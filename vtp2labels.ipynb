{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feat_matrix \n",
    "\n",
    "1. reduce the size of each feature matrix for each .vtp fiber bundle (basis of a percentage 10)\n",
    "2. attach label to each feat_matrix.h5 file using label mapping\n",
    "3. combine the 54 tracts mentioned in the supplementary of the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: <KeysViewHDF5 ['feat']>\n",
      "44656\n",
      "30\n",
      "10\n",
      "<class 'h5py._hl.dataset.Dataset'>\n",
      "2232\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "AF_left_feat_path = \"/media/ang/Data/unnerve_data/779370/779370_T_AF_left_featMatrix.h5\"\n",
    "\n",
    "# read feat_matrix\n",
    "with h5py.File(AF_left_feat_path, \"r\") as f:\n",
    "        # List all groups\n",
    "        print(\"Keys: %s\" % f.keys())\n",
    "        h_keys = list(f.keys())[0]\n",
    "\n",
    "        # Get the data\n",
    "        data = list(f[h_keys])\n",
    "        print(len(data))\n",
    "        print(len(f[h_keys][0]))\n",
    "\n",
    "        print(len(f[h_keys][0][0:10]))\n",
    "        print(type(f[h_keys]))\n",
    "        print(int(len(f[h_keys])/20))\n",
    "        compressed_data = f[h_keys][0: int(len(f[h_keys]))]\n",
    "        print(type(compressed_data))\n",
    "        \n",
    "\n",
    "# reduce the size of feature_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('S14')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_mapping_72 = pd.read_csv(\"/home/ang/Documents/GitHub/DeepWMA/labels_reference.csv\", names=[\"label_names\", \"label_array\"])\n",
    "# label_mapping_54 =  pd.read_csv(\"/home/ang/Documents/GitHub/DeepWMA/labels_reference_54.csv\", names=[\"label_names\", \"label_array\"])\n",
    "\n",
    "label_mapping_72['label_array'].dtype\n",
    "label_mapping_72['label_names'] = label_mapping_72['label_names'].astype('|S')\n",
    "label_mapping_72['label_names'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feat_array = np.random.random(size=(44656, 30, 30, 3))\n",
    "h5f = h5py.File('data.h5', 'w')\n",
    "\n",
    "label_hdf_dtype = np.dtype([('label_names', str), ('label_values', int), ('label_array', int)])\n",
    "\n",
    "subject_iD=\"779370\"\n",
    "# pass this subject ID from the bash file code\n",
    "output_folder=\"/media/ang/Data/unnerve_data/\".format(subject_ID)\n",
    "fiber_tract='T_AF_LEFT'\n",
    "# extract fiber tract name from the path of the tract\n",
    "# either search for a list of names in the whole string path (find and replace type)\n",
    "\n",
    "feat_matrix= \"/media/ang/Data/unnerve_data/779370/779370_T_AF_left_featMatrix.h5\"\n",
    "\n",
    "with h5py.File('label_{}.h5'.format(fiber_tract),'w') as h5f:\n",
    "    grp = h5f.create_group(fiber_tract)\n",
    "\n",
    "    # iterate over all fiber tracts\n",
    "\n",
    "    with h5py.File(feat_matrix, 'w') as feath5f: \n",
    "        h_keys = list(feath5f.keys())[0]\n",
    "        feath5f[h_keys]\n",
    "\n",
    "        # limit the size of the numpy array to 5% of the array so that the error of big size can be resolved \n",
    "\n",
    "        feat_label=np.empty(dtype=label_hdf_dtype, shape)\n",
    "\n",
    "        # concatenate this feath5[h_keys] with a column of feat_matric.shape[0], so that values of label\n",
    "        # can be assigned to each feat tract \n",
    "        grp.create_dataset(fiber_tract, data=feath5f[h_keys])\n",
    "\n",
    "\n",
    "        ## also create a separate label file for the size feat_matrix.shape[0]\n",
    "\n",
    "    \n",
    "    # combine all fiber tract with 5% data \n",
    "    # combine all labels (or keep appending the values)\n",
    "    \n",
    "    h5f.create_dataset('label{}'.format, data=feat_label)\n",
    "\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: <KeysViewHDF5 ['dataset_1']>\n",
      "44656\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## labels\n",
    "\n",
    "1. after step 2 above\n",
    "2. combine the labels for each tract with limited fibers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 ('SupWMA')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "36aec90b5a07fafc043f146ab1db38cbda9ca0590150a6d49f81c4cf72882f5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
